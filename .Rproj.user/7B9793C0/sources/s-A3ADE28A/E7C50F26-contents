library(kwb.odm)
library(kwb.utils)
#library(kwb.ogre)
require(dplyr)

get_lab_values <- function # Reads all lab values from ODBC-source
### Appends also site code (e.g., "NEU") and substance name
### apart from standard-fields the fields "CensorCode", "QualityControlLevelID",
### and "UnitsAbbreviation" are included
### If one measurements exists twice, only higher QualityControlLevelID is included
(
  odbc_name
  ### Name of the odbc source
)
{
  # get info from db

  sites <- odm_Sites(db=odbc_name, select=1:3, stringsAsFactors = FALSE)

  substances <- odm_Variables(db=odbc_name, stringsAsFactors = FALSE, select=c(1:3,5))

  samples <- odm_Samples(db=odbc_name, select=1:2, stringsAsFactors = FALSE)

  datavalues <- odm_DataValues(db=odbc_name, select = "ValueID, DataValue,
                               LocalDateTime, DateTimeUTC, UTCOffset,
                               SampleID, SiteID, VariableID,
                               CensorCode, QualityControlLevelID",
                               orderBy_QualityControlLevelID = 1,
                               stringsAsFactors = FALSE,
                               as.is = TRUE # keep dates and times as character
                               )

  # Explicitly convert character to POSIXct
  datavalues$LocalDateTime <- as.POSIXct(datavalues$LocalDateTime, tz = "Europe/Berlin")
  datavalues$DateTimeUTC <- as.POSIXct(datavalues$DateTimeUTC, tz = "UTC")

  units_variables <- odm_Units(db=odbc_name, stringsAsFactors = FALSE, select=c(1,4))

  # merge info in one data.frame

  x_all <- merge(datavalues, sites, by ="SiteID")

  x_all <- merge(x_all, substances, by ="VariableID")

  x_all <- merge(x_all, units_variables, by.x="VariableUnitsID", by.y="UnitsID")

  x_all <- merge(x_all, samples, by ="SampleID")

  # as merged dataframe x_all is not sorted by QualityControlLevelID anymore, do it again
  x_all <- x_all[order(x_all$QualityControlLevelID), ]

  # keep only higher QualityControlLevelID if measurements are double
  y <- aggregate(ValueID ~ SampleID + VariableID, data = x_all, FUN = length)
  indices <- which(y[,3] > 1)

  y2 <- aggregate(ValueID ~ SampleID + VariableID, data = x_all, FUN = "[", 1)

  ValueIDs_LQ <- y2$ValueID[indices]
  indices_LQ <- match(ValueIDs_LQ, x_all$ValueID)

  x_all[-indices_LQ,]
}



only_new_dl_metals <- function #removes metal samples below detection limit (dl),
### when dl was too high (old analytical method). Works by VariableCode
(
  x_in
  ### name of input data.frame
)
{
  # get detection limits that were lowered during project
  DL_metals_new <- getNewDetectionLimits()

  # delete samples below detection limit, where detection limit is higher than current
  for(i in 1:length(DL_metals_new$VariableCode)) {

    indices <- which(x_in$VariableCode == DL_metals_new[i,1] &
                       x_in$CensorCode == "lt" &
                       x_in$DataValue > DL_metals_new[i,2])

    if (length(indices) > 0) {

      x_in <- x_in[-indices,]

    }

  }

  x_in

}

only_composite <- function # removes rows in data.frame with SampleType ! = "Composite"
(
  x_in
  ### name of input data.frame
)
{
  indices <- which(x_in$SampleType == "Composite")
  x_in[indices,]
}

remove_group <- function # removes measurements of Variables of a specific group
(
  x_in,
  ### name of input data.frame
  group
  ### Variable group to be removed (string)
)
{
  ogreVariables <- OGRE_VARIABLES()

  ogreVariableCodes <- ogreVariables$VariableCode[
    ogreVariables$VariableGroupName == paste(group)]

  indices <- which(x_in$VariableCode %in% ogreVariableCodes == "FALSE")

  x_in[indices,]
}

no_Panke <- function # removes rows in data.frame with site code = "PNK"
(
  x_in
  ### name of input data.frame
)
{
  indices <- which(x_in$SiteCode != "PNK")

  x_in[indices,]
}

Panke <- function # keeps only rows in data.frame with site code = "PNK"
(
  x_in
  ### name of input data.frame
)
{
  indices <- which(x_in$SiteCode == "PNK")

  x_in[indices,]
}

reduce_codes <- function # removes lines with censor codes
### other than "lt" and "nc" (e.g., "???" are removed)
(
  x_in
  ### name of input data.frame
)
{
  indices_OK <- which(x_in$CensorCode %in% c("lt", "nc"))

  x_in[indices_OK,]
}

only_representative_subst <- function # removes Variables, which have not
### at least one measurement (can also be below detection limit) per catchment type
(
  x_in
  ### name of input data.frame
)
{
  VariableIDs <- unique(x_in$VariableID)
  siteIDs <- unique(x_in$SiteID)

  for (i in 1:length(VariableIDs)) {
    indices <- which(x_in$VariableID == VariableIDs[i])
    site_match <- siteIDs %in% x_in$SiteID[indices]
    if (sum(site_match) < length(siteIDs)) {
      x_in <- x_in[-indices,]
    }
  }

  x_in
}


non_detect <- function # lists substances without detection in any sample
### Apart from entire dataset x_in (first column),
### lists are given for each site individually (following columns)
(
  x_in
  ### name of input data.frame
)
{
  site_IDs <- unique(x_in$SiteID)
  y <- order(site_IDs)
  site_IDs <- site_IDs[y]

  mat_nodetect <- matrix(nrow=200, ncol=length(site_IDs)+1)

  #find substances which are < dl in all samples
  indices_lt <- which(x_in$CensorCode == "lt")
  subst_lt <- unique(x_in$VariableName[indices_lt])
  subst_nc <- unique(x_in$VariableName[-indices_lt])
  subst_lt <- setdiff(subst_lt,subst_nc)
  mat_nodetect[1:length(subst_lt),1] <- subst_lt
  max_length <- length(subst_lt)

  #find substances which are < dl for all samples of one site

  for (i in 1:length(site_IDs)) {
    indices_lt <- which(x_in$CensorCode == "lt" & x_in$SiteID == site_IDs[i])
    indices_nc <- which(x_in$CensorCode == "nc" & x_in$SiteID == site_IDs[i])
    subst_lt <- unique(x_in$VariableName[indices_lt])
    subst_nc <- unique(x_in$VariableName[indices_nc])
    subst_lt <- setdiff(subst_lt,subst_nc)
    mat_nodetect[1:length(subst_lt),i+1] <- subst_lt
    max_length <- max(length(subst_lt), max_length)
  }

  mat_nodetect <- mat_nodetect[1:max_length,]

  tab_non_detect <- as.data.frame(mat_nodetect, stringsAsFactors=FALSE)

  indices <- match(site_IDs, x_in$SiteID)
  site_names <- x_in$SiteCode[indices]

  colnames(tab_non_detect) <- c("All_sites", site_names[1:length(site_names)])

  tab_non_detect
}

detect <- function # removes substances, without detection from data.frame
### requires list of these substances as single vector
(
  x_in,
  ### name of input data.frame
  x_nd
  ### vector with substances < detection
)
{
  y <- match(x_in$VariableName, x_nd)
  indices <- which(is.na(y))
  x_in[indices,]
}

adapt_nondetect <- function # adapts values of single results < detection limit.
### requires list of these substances as data.frame for each site
### (one column per site). If substance is always < dl at one site,
### results are set to zero.
### If substance is sometimes > dl, resluts are set to a factor*dl
(
  x_in,
  ### name of input data.frame
  x_nd,
  ### vector with substances < detection (one column per site)
  factor = 0.5
  ### multiplier of detection limit if smaller dl (e.g., 0, 0.5 or 1)
)
{
  site_names <- colnames(x_nd)[-1]

  for (i in 1:(dim(x_nd)[2]-1)) {
    #set samples <dl for substances which are never detected = 0
    y <- match(x_in$VariableName, x_nd[,i+1])
    indices <- which(x_in$SiteCode == site_names[i] & y > 0)
    x_in$DataValue[indices] <- 0
  }

  #set samples <dl for substances which are partially detected = factor*dl
  indices <- which(x_in$CensorCode == "lt" & x_in$DataValue != 0)
  x_in$DataValue[indices] <- factor * x_in$DataValue[indices]

  x_in
}

annual_mean_conc <- function # estimates annual mean concentrations per site.
### Apart from a matrix with mean concentrations for each substance and site
### (= 0 if always below detection limit), matrices with N, standard error,
### standard deviation, RMSE, as well as measured min and max are calculated.
### Result is given as a list, of these matrices.
### Different methods can be chosen:
### Method 1: arithmetic mean
### Method 2: functions and RMSE from file, arithmetic mean for substances without functions
(
  x_in,
  ### name of input data.frame
  method,
  ### estimation method:
  ### 1 = arithmetic mean
  ### 2 = functions and RMSE from file, arithmetic mean for substances without functions
  data.dir
  ### file directory where correlation data and rain series for method 2 are located
)
{
  #order by VariableID
  y <- order(x_in$VariableID)
  x_in <- x_in[y,]

  #find existing sites
  site_IDs <- unique(x_in$SiteID)
  y <- order(site_IDs)
  site_IDs <- site_IDs[y]
  indices <- match(site_IDs, x_in$SiteID)
  site_names <- x_in$SiteCode[indices]

  #output structure
  x_out_mean <- data.frame(VariableID=unique(x_in$VariableID),
                           VariableName=unique(x_in$VariableName),
                           stringsAsFactors=FALSE)
  indices <- match(x_out_mean$VariableID, x_in$VariableID)
  x_out_mean$UnitsAbbreviation <- x_in$UnitsAbbreviation[indices]

  x_out_stdev <- x_out_mean


  if (method == 1) {

    #apply method 1 (arithmetic mean for all sites)
    x_out <- annual_mean_conc_method1(x_in = x_in, x_out_mean = x_out_mean,
                                   x_out_stdev = x_out_stdev, site_names = site_names)

  } else if (method == 2) {

    #apply method 1 (arithmetic mean for all sites), 1st step
    x_out_method1 <- annual_mean_conc_method1(x_in = x_in, x_out_mean = x_out_mean,
                                   x_out_stdev = x_out_stdev, site_names = site_names)

    #apply method 2 (replace method 1 where correlations exist)

    x_out <- annual_mean_conc_method2(x_out_mean = x_out_method1$mean,
                                      x_out_stdev = x_out_method1$stdev,
                                      site_names = site_names,
                                      data.dir = data.dir)


    } else {
    paste("method is not established yet")
  }

  x_out
}

# default_statistics -----------------------------------------------------------
default_statistics <- function(x) # calculate default statistics
### calculate default statistics for a grouped data frame (created with
### dplyr::group_by)
{
  x %>% summarise(
    mean = mean(DataValue),
    N = length(DataValue),
    N_lt = sum(CensorCode == "lt"),
    N_nc = sum(CensorCode == "nc"),
    stdev = sd(DataValue),
    var = var(DataValue),
    se = sd(DataValue) / sqrt(length(DataValue) - 1),
    min = min(DataValue),
    max = max(DataValue),
    median = median(DataValue),
    quantile2_5 = quant2_5(DataValue),
    quantile5 = quant5(DataValue),
    quantile25 = quant25(DataValue),
    quantile75 = quant75(DataValue),
    quantile95 = quant95(DataValue),
    quantile97_5 = quant97_5(DataValue),
  )
}

# annual_stats -----------------------------------------------------------------
annual_stats <- function # estimates annual mean concentrations per site.
### Apart from a matrix with mean concentrations for each substance and site
### (= 0 if always below detection limit), matrices with N, 95% confidence,
### as well as measured min and max are calculated. Result is given as a list,
### of these matrices.
(
  x_in
  ### name of input data.frame
)
{
  # Provide vector of SiteCodes ordered by their SiteID
  site_names <- (x_in %>% group_by(SiteID, SiteCode) %>% summarise())$SiteCode

  # Provide statistics grouped by SiteCode and Variable
  x_summary <- x_in %>%
    group_by(SiteCode, VariableID, VariableName, UnitsAbbreviation) %>%
    default_statistics()

  # Provide statistics grouped by Variable only
  x_total <- x_in %>%
    group_by(VariableID, VariableName, UnitsAbbreviation) %>%
    default_statistics()

  # Provide vectors of column names
  columns.variable <- c("VariableID", "VariableName", "UnitsAbbreviation")
  columns.key <- c("SiteCode", columns.variable)
  columns.stat <- setdiff(names(x_summary), columns.key)

  x_out <- list()

  for (column in columns.stat) {

    # Reshape from "long" to "wide"
    result <- reshape(
      data = as.data.frame(x_summary[, c(columns.key, column)]),
      direction = "wide",
      idvar = columns.variable,
      timevar = "SiteCode"
    )

    # Remove attribute given by reshape
    attr(result, "reshapeWide") <- NULL

    # Remove column name prefixes given by rehape
    names(result) <- gsub(paste0("^", column, "\\."), "", names(result))

    # Merge site statistics with totals
    x <- result[, c(columns.variable, site_names)]

    y <- x_total[, c(columns.variable, column)]
    names(y)[ncol(y)] <- "Gesamt"

    result <- merge(x, y) %>% arrange(VariableID)

    x_out[[column]] <- result
  }

  x_out
}

annual_load_rain <- function # calculates the load for each substance
### separates pathways (rain runoff, CSO and WWTP)
(
  data.dir,
  ### path of model data (annual mean concentrations "annual_mean_conc.csv",
  ### rain runoff volumes "Vol_rain.csv,
  ### removal at WWTP "removal_RW_sub.csv")
  error_removal_rate = 0.3,
  ### relative error in removal at WWTP
  percent = NULL
)
{

  #load data

  if (file.exists(file.path(data.dir, "Vol_rain.csv"))) {
  vol_rain <- read.csv2(file=file.path(data.dir, "Vol_rain.csv"), stringsAsFactors=FALSE)
  } else stop("File with rain runoff (Vol_rain.csv) not found in data.dir")

  if (file.exists(file.path(data.dir, "Vol_rain_relative_error.csv"))) {
    error_vol_rain <- read.table(file = file.path(data.dir, "Vol_rain_relative_error.csv"), sep = ";", dec = ".", stringsAsFactors=FALSE, header = TRUE)
  } else stop("File with rain runoff (Vol_rain_relative_error.csv) not found in data.dir")

  if (file.exists(file.path(data.dir, "annual_mean_conc.csv"))) {
  x_conc <- read.table(file = file.path(data.dir, "annual_mean_conc.csv"), sep = ";", dec = ".", stringsAsFactors=FALSE, header = TRUE)
  } else stop("File with annual mean concentrations (annual_mean_conc.csv) not found in data.dir")


  ###

  mypath <- sprintf("annual_mean_conc_wrongcon%d.csv", percent)

  if (file.exists(file.path(data.dir, mypath))) {
    x_conc_wrongcon <- read.table(file = file.path(data.dir, mypath), sep = ";", dec = ".", stringsAsFactors=FALSE, header = TRUE)
  } else stop("File with annual mean concentrations with wrong connections (annual_mean_conc_wrongcon.csv) not found in data.dir")


  ###

  if (file.exists(file.path(data.dir, "annual_mean_conc_relative_error.csv"))) {
    error_x_conc <- read.table(file = file.path(data.dir, "annual_mean_conc_relative_error.csv"), sep = ";", dec = ".", stringsAsFactors=FALSE, header = TRUE)
  } else stop("File with annual mean concentrations (annual_mean_conc_relative_error.csv) not found in data.dir")

  if (file.exists(file.path(data.dir, "annual_mean_conc_relative_error_wrongcon.csv"))) {
    error_x_conc_wrongcon <- read.table(file = file.path(data.dir, "annual_mean_conc_relative_error_wrongcon.csv"), sep = ";", dec = ".", stringsAsFactors=FALSE, header = TRUE)
  } else stop("File with annual mean concentrations with wrong connections (annual_mean_conc_relative_error_wrongcon.csv) not found in data.dir")

  if (file.exists(file.path(data.dir, "substance_info.csv"))) {
  removal_rates <- read.csv2(file=file.path(data.dir, "substance_info.csv"), header=TRUE, stringsAsFactors=FALSE)
  } else stop("File with removal rates at WWTP (substance_info.csv) not found in data.dir")

  # missing removal rates are set = 0
  removal_rates[,2] <- as.numeric(removal_rates[,2])
  removal_rates[which(is.na(removal_rates$Retention_.)),2] <- 0

  # get removal rates for substances in x_conc only (and in same order)
  removal_rates_red <- x_conc[,1:2]
  indices <- match(removal_rates_red$VariableName, removal_rates$VariableName)
  removal_rates_red$removal_percent <- removal_rates$Retention_.[indices]

  # order of catchments
  sum_EZG <- aggregate(vol_rain$GESAMT, by = list(vol_rain$SUW), FUN = "sum")
  indices <- order(sum_EZG$x, decreasing = TRUE)
  sum_EZG <- sum_EZG[indices,]
  EZG_names <- sum_EZG[,1]

  # structure of calculation files
  load_sep <- x_conc[,c(1,2,4:9)]
  load_sep[,3:8] <- 0
  load_sep$TOT <- 0
  error_load_sep <- load_sep
  load_CSO <- load_sep
  error_load_CSO <- load_sep
  load_WWTP <- load_sep
  error_load_WWTP <- load_sep

  # output list
  x_out_by_catchment_kg_yr <- list()
  error_by_catchment_kg_yr <- list()
  x_out_by_pathway_kg_yr <- list()
  error_by_pathway_kg_yr <- list()

  for (i in 1:length(EZG_names)) {

    indices <- which(vol_rain$SUW == EZG_names[i])
    vol_rain_EZG <- vol_rain[indices,]
    error_vol_rain_EZG <- error_vol_rain[indices,]

    # loads of rain-water based substances via separate sewer system
    load_sep$ALT <- x_conc_wrongcon$ALT_Fehl * vol_rain_EZG$ALT[1]
    load_sep$NEU <- x_conc_wrongcon$NEU_Fehl * vol_rain_EZG$NEU[1]
    load_sep$STR <- x_conc_wrongcon$STR_Fehl * vol_rain_EZG$STR[1]
    load_sep$EFH <- x_conc_wrongcon$EFH_Fehl * vol_rain_EZG$EFH[1]
    load_sep$GEW <- x_conc_wrongcon$GEW_Fehl * vol_rain_EZG$GEW[1]
    load_sep$ANDERE <- x_conc_wrongcon$ANDERE_Fehl * vol_rain_EZG$ANDERE[1]
    load_sep$TOT <- load_sep$ALT + load_sep$NEU + load_sep$STR + load_sep$EFH +
      load_sep$GEW + load_sep$ANDERE

    # absolute errors in loads of rain-water based substances via separate sewer system
    error_load_sep$ALT <- sqrt(error_x_conc_wrongcon$ALT_Fehl^2 + error_vol_rain_EZG$ALT[1]^2)*load_sep$ALT
    error_load_sep$NEU <- sqrt(error_x_conc_wrongcon$NEU_Fehl^2 + error_vol_rain_EZG$NEU[1]^2)*load_sep$NEU
    error_load_sep$STR <- sqrt(error_x_conc_wrongcon$STR_Fehl^2 + error_vol_rain_EZG$STR[1]^2)*load_sep$STR
    error_load_sep$EFH <- sqrt(error_x_conc_wrongcon$EFH_Fehl^2 + error_vol_rain_EZG$EFH[1]^2)*load_sep$EFH
    error_load_sep$GEW <- sqrt(error_x_conc_wrongcon$GEW_Fehl^2 + error_vol_rain_EZG$GEW[1]^2)*load_sep$GEW
    error_load_sep$ANDERE <- sqrt(error_x_conc_wrongcon$ANDERE_Fehl^2 + error_vol_rain_EZG$ANDERE[1]^2)*load_sep$ANDERE
    error_load_sep$TOT <- sqrt(error_load_sep$ALT^2 + error_load_sep$NEU^2 + error_load_sep$STR^2 + error_load_sep$EFH^2 +
      error_load_sep$GEW^2 + error_load_sep$ANDERE^2)


    # loads of rain-water based substances via CSO
    load_CSO$ALT <- x_conc$ALT * vol_rain_EZG$ALT[2]
    load_CSO$NEU <- x_conc$NEU * vol_rain_EZG$NEU[2]
    load_CSO$STR <- x_conc$STR * vol_rain_EZG$STR[2]
    load_CSO$EFH <- x_conc$EFH * vol_rain_EZG$EFH[2]
    load_CSO$GEW <- x_conc$GEW * vol_rain_EZG$GEW[2]
    load_CSO$ANDERE <- x_conc$ANDERE * vol_rain_EZG$ANDERE[2]
    load_CSO$TOT <- load_CSO$ALT + load_CSO$NEU + load_CSO$STR + load_CSO$EFH +
      load_CSO$GEW + load_CSO$ANDERE

    # absolute errors in loads of rain-water based substances via CSO
    error_load_CSO$ALT <- sqrt(error_x_conc$ALT^2 + error_vol_rain_EZG$ALT[2]^2)*load_CSO$ALT
    error_load_CSO$NEU <- sqrt(error_x_conc$NEU^2 + error_vol_rain_EZG$NEU[2]^2)*load_CSO$NEU
    error_load_CSO$STR <- sqrt(error_x_conc$STR^2 + error_vol_rain_EZG$STR[2]^2)*load_CSO$STR
    error_load_CSO$EFH <- sqrt(error_x_conc$EFH^2 + error_vol_rain_EZG$EFH[2]^2)*load_CSO$EFH
    error_load_CSO$GEW <- sqrt(error_x_conc$GEW^2 + error_vol_rain_EZG$GEW[2]^2)*load_CSO$GEW
    error_load_CSO$ANDERE <- sqrt(error_x_conc$ANDERE^2 + error_vol_rain_EZG$ANDERE[2]^2)*load_CSO$ANDERE
    error_load_CSO$TOT <- sqrt(error_load_CSO$ALT^2 + error_load_CSO$NEU^2 + error_load_CSO$STR^2 + error_load_CSO$EFH^2 +
                                 error_load_CSO$GEW^2 + error_load_CSO$ANDERE^2)


    # loads of rain-water based substances via WWTP
    load_WWTP$ALT <- x_conc$ALT * vol_rain_EZG$ALT[3] * (1-removal_rates_red$removal_percent/100)
    load_WWTP$NEU <- x_conc$NEU * vol_rain_EZG$NEU[3] * (1-removal_rates_red$removal_percent/100)
    load_WWTP$STR <- x_conc$STR * vol_rain_EZG$STR[3] * (1-removal_rates_red$removal_percent/100)
    load_WWTP$EFH <- x_conc$EFH * vol_rain_EZG$EFH[3] * (1-removal_rates_red$removal_percent/100)
    load_WWTP$GEW <- x_conc$GEW * vol_rain_EZG$GEW[3] * (1-removal_rates_red$removal_percent/100)
    load_WWTP$ANDERE <- x_conc$ANDERE * vol_rain_EZG$ANDERE[3] * (1-removal_rates_red$removal_percent/100)
    load_WWTP$TOT <- load_WWTP$ALT + load_WWTP$NEU + load_WWTP$STR + load_WWTP$EFH +
      load_WWTP$GEW + load_WWTP$ANDERE

    # error in loads of rain-water based substances via WWTP
    error_load_WWTP$ALT <- sqrt(error_x_conc$ALT^2 + error_vol_rain_EZG$ALT[3]^2 + error_removal_rate^2)*load_WWTP$ALT
    error_load_WWTP$NEU <- sqrt(error_x_conc$NEU^2 + error_vol_rain_EZG$NEU[3]^2 + error_removal_rate^2)*load_WWTP$NEU
    error_load_WWTP$STR <- sqrt(error_x_conc$STR^2 + error_vol_rain_EZG$STR[3]^2 + error_removal_rate^2)*load_WWTP$STR
    error_load_WWTP$EFH <- sqrt(error_x_conc$EFH^2 + error_vol_rain_EZG$EFH[3]^2 + error_removal_rate^2)*load_WWTP$EFH
    error_load_WWTP$GEW <- sqrt(error_x_conc$GEW^2 + error_vol_rain_EZG$GEW[3]^2 + error_removal_rate^2)*load_WWTP$GEW
    error_load_WWTP$ANDERE <- sqrt(error_x_conc$ANDERE^2 + error_vol_rain_EZG$ANDERE[3]^2 + error_removal_rate^2)*load_WWTP$ANDERE
    error_load_WWTP$TOT <- sqrt(error_load_WWTP$ALT^2 + error_load_WWTP$NEU^2 + error_load_WWTP$STR^2 + error_load_WWTP$EFH^2 +
                                 error_load_WWTP$GEW^2 + error_load_WWTP$ANDERE^2)

    # load unit for all substances in kg/yr, MPN/yr, PFU/yr, GC/yr
    indices <- which(x_conc$UnitsAbbreviation == "mg/L")
    load_sep[indices,-(1:2)] <- load_sep[indices,-(1:2)] / 1000
    load_CSO[indices,-(1:2)] <- load_CSO[indices,-(1:2)] / 1000
    load_WWTP[indices,-(1:2)] <- load_WWTP[indices,-(1:2)] / 1000
    error_load_sep[indices,-(1:2)] <- error_load_sep[indices,-(1:2)] / 1000
    error_load_CSO[indices,-(1:2)] <- error_load_CSO[indices,-(1:2)] / 1000
    error_load_WWTP[indices,-(1:2)] <- error_load_WWTP[indices,-(1:2)] / 1000

    indices2 <- which(x_conc$UnitsAbbreviation %in% c("MPN/100 mL", "PFU/100 mL"))
    load_sep[indices2,-(1:2)] <- load_sep[indices2,-(1:2)] * 10000
    load_CSO[indices2,-(1:2)] <- load_CSO[indices2,-(1:2)] * 10000
    load_WWTP[indices2,-(1:2)] <- load_WWTP[indices2,-(1:2)] * 10000
    error_load_sep[indices2,-(1:2)] <- error_load_sep[indices2,-(1:2)] * 10000
    error_load_CSO[indices2,-(1:2)] <- error_load_CSO[indices2,-(1:2)] * 10000
    error_load_WWTP[indices2,-(1:2)] <- error_load_WWTP[indices2,-(1:2)] * 10000

    indices3 <- which(x_conc$UnitsAbbreviation %in% c("PFU/L", "GC/L"))
    load_sep[indices3,-(1:2)] <- load_sep[indices3,-(1:2)] * 1000
    load_CSO[indices3,-(1:2)] <- load_CSO[indices3,-(1:2)] * 1000
    load_WWTP[indices3,-(1:2)] <- load_WWTP[indices3,-(1:2)] * 1000
    error_load_sep[indices3,-(1:2)] <- error_load_sep[indices3,-(1:2)] * 1000
    error_load_CSO[indices3,-(1:2)] <- error_load_CSO[indices3,-(1:2)] * 1000
    error_load_WWTP[indices3,-(1:2)] <- error_load_WWTP[indices3,-(1:2)] * 1000

    indices4 <- dplyr::union(indices, indices2)
    indices5 <- dplyr::union(indices3, indices4)
    load_sep[-indices5,-(1:2)] <- load_sep[-indices5,-(1:2)] / 1e6
    load_CSO[-indices5,-(1:2)] <- load_CSO[-indices5,-(1:2)] / 1e6
    load_WWTP[-indices5,-(1:2)] <- load_WWTP[-indices5,-(1:2)] / 1e6
    error_load_sep[-indices5,-(1:2)] <- error_load_sep[-indices5,-(1:2)] / 1e6
    error_load_CSO[-indices5,-(1:2)] <- error_load_CSO[-indices5,-(1:2)] / 1e6
    error_load_WWTP[-indices5,-(1:2)] <- error_load_WWTP[-indices5,-(1:2)] / 1e6

    # summary by pathway
    load_summary_path <- x_conc[,1:2]
    load_summary_path$sep <- load_sep$TOT
    load_summary_path$CSO <- load_CSO$TOT
    load_summary_path$WWTP <- load_WWTP$TOT
    load_summary_path$TOT <- load_sep$TOT + load_CSO$TOT + load_WWTP$TOT

    #absolute error by pathway
    error_load_summary_path <- load_summary_path
    error_load_summary_path$sep <- error_load_sep$TOT
    error_load_summary_path$CSO <- error_load_CSO$TOT
    error_load_summary_path$WWTP <- error_load_WWTP$TOT
    error_load_summary_path$TOT <- sqrt(error_load_sep$TOT^2 + error_load_CSO$TOT^2 + error_load_WWTP$TOT^2)

    # summary by source catchment
    load_summary_catch <- load_sep
    load_summary_catch[,3:9] <- load_sep[,3:9] + load_CSO[,3:9] + load_WWTP[,3:9]

    # absolute error by source catchment
    error_load_summary_catch <- error_load_sep
    error_load_summary_catch[,3:9] <- sqrt(error_load_sep[,3:9]^2 + error_load_CSO[,3:9]^2 + error_load_WWTP[,3:9]^2)

    # organize in list
    x_out_by_catchment_kg_yr[[EZG_names[i]]] <- load_summary_catch
    x_out_by_pathway_kg_yr[[EZG_names[i]]] <- load_summary_path
    error_by_catchment_kg_yr[[EZG_names[i]]] <- error_load_summary_catch
    error_by_pathway_kg_yr[[EZG_names[i]]] <- error_load_summary_path

  }


  # output
  list(by_path = x_out_by_pathway_kg_yr,
       error_by_path = error_by_pathway_kg_yr,
       by_catch = x_out_by_catchment_kg_yr,
       error_by_catch = error_by_catchment_kg_yr)

}


annual_load_sewage <- function # calculates the load for each substance
### separates pathways (CSO and WWTP)
(
  data.dir,
  ### path of model data
  error_removal_rate = 0.3,
  ### relative error in removal at WWTP
  error_conc = 0.5
  ### relative error in concentrations at WWTP
)
{

  #load data

  if (file.exists(file.path(data.dir, "Vol_sewage.csv"))) {
    vol_sewage <- read.table(file = file.path(data.dir, "Vol_sewage.csv"), sep = ";", dec = ".", stringsAsFactors=FALSE, header = TRUE)
  } else stop("File with sewage runoff (Vol_sewage.csv) not found in data.dir")

  if (file.exists(file.path(data.dir, "Vol_sewage_relative_error.csv"))) {
    error_vol_sewage <- read.table(file = file.path(data.dir, "Vol_sewage_relative_error.csv"), sep = ";", dec = ".", stringsAsFactors=FALSE, header = TRUE)
  } else stop("File with sewage runoff (Vol_sewage_relative_error.csv) not found in data.dir")

  if (file.exists(file.path(data.dir, "substance_info.csv"))) {
    sub_sew_info <- read.csv2(file=file.path(data.dir, "substance_info.csv"), header=TRUE, stringsAsFactors=FALSE)
  } else stop("File with substance information WWTP (substance_info.csv) not found in data.dir")

  # read substance information, discard substances with lacking info
  sub_sew_info$CinWWTP_calculated <- as.numeric(sub_sew_info$CinWWTP_calculated)
  sub_sew_info$Retention_. <- as.numeric(sub_sew_info$Retention_.)

  selected <- !is.na(sub_sew_info$CinWWTP_calculated)
  sub_sew_info <- sub_sew_info[selected,]

  #set retention to zero, where information is lacking
  indices <- which(is.na(sub_sew_info$Retention_.))
  sub_sew_info$Retention_.[indices] <- 0

  # order of catchments
  sum_EZG <- aggregate(vol_sewage$GESAMT, by = list(vol_sewage$SUW), FUN = "sum")
  indices <- order(sum_EZG$x, decreasing = TRUE)
  sum_EZG <- sum_EZG[indices,]
  EZG_names <- sum_EZG[,1]

  # structure of calculation file
  load_sew <- data.frame(VariableName = sub_sew_info$VariableName,
                         CSO = 0, WWTP = 0, TOT = 0, stringsAsFactors=FALSE)
  error_load_sew <- load_sew

  # output list
  x_out_by_pathway_kg_yr <- list()
  error_by_pathway_kg_yr <- list()

  for (i in 1:length(EZG_names)) {

    indices <- which(vol_sewage$SUW == EZG_names[i])
    vol_sewage_EZG <- vol_sewage[indices,]
    error_vol_sewage_EZG <- error_vol_sewage[indices,]

    # loads of sewage based substances per pathway
    load_sew$CSO <- vol_sewage_EZG$GESAMT[1] * sub_sew_info$CinWWTP_calculated
    load_sew$WWTP <- vol_sewage_EZG$GESAMT[2] * sub_sew_info$CinWWTP_calculated * (1-sub_sew_info$Retention_./100)
    load_sew$TOT <- load_sew$CSO + load_sew$WWTP

    # absolute errors in loads of sewage based substances per pathway
    error_load_sew$CSO <- sqrt(error_vol_sewage_EZG$GESAMT[1]^2 + error_conc^2)*load_sew$CSO
    error_load_sew$WWTP <- sqrt(error_vol_sewage_EZG$GESAMT[2]^2 + error_conc^2 + error_removal_rate^2)*load_sew$WWTP
    error_load_sew$TOT <- sqrt(error_load_sew$CSO^2 + error_load_sew$WWTP^2)

    # load unit for all substances in kg/yr, MPN/yr, PFU/yr
    indices <- which(sub_sew_info$UnitsAbbreviation == "mg/L")
    load_sew[indices,-1] <- load_sew[indices,-1] / 1000
    error_load_sew[indices,-1] <- error_load_sew[indices,-1] / 1000

    indices2 <- which(sub_sew_info$UnitsAbbreviation %in% c("MPN/100 mL", "PFU/100 mL"))
    load_sew[indices2,-1] <- load_sew[indices2,-1] * 10000
    error_load_sew[indices2,-1] <- error_load_sew[indices2,-1] * 10000

    indices3 <- which(sub_sew_info$UnitsAbbreviation %in% c("PFU/L", "GC/L"))
    load_sew[indices3,-1] <- load_sew[indices3,-1] * 1000
    error_load_sew[indices3,-1] <- error_load_sew[indices3,-1] * 1000

    indices4 <- dplyr::union(indices,indices2)
    indices5 <- dplyr::union(indices3,indices4)
    load_sew[-indices5,-1] <- load_sew[-indices5,-1] / 1e6
    error_load_sew[-indices5,-1] <- error_load_sew[-indices5,-1] / 1e6

    # organize in list
    x_out_by_pathway_kg_yr[[EZG_names[i]]] <- load_sew
    error_by_pathway_kg_yr[[EZG_names[i]]] <- error_load_sew

  }

  # output
  list(by_path = x_out_by_pathway_kg_yr,
       error_by_path = error_by_pathway_kg_yr)

}


getNewDetectionLimits <- function #get detection limits for variables
### that changed (lowered) during the monitoring
()
{
  detectionLimits.vector <- c(
    Pb = 0.5,
    Cd = 0.05,
    Cr  = 0.2,
    Ni  = 0.5,
    V  = 0.1
  )

  detectionLimits <- data.frame(
    VariableCode = names(detectionLimits.vector),
    DetectionLimit = as.numeric(detectionLimits.vector),
    stringsAsFactors = FALSE
  )
  ### data frame with columns VariableCode, DetectionLimit
}

quant2_5<-function(x)
  ## function gives the 25% quantile
{
  a<-quantile(x, c(0.025))
  return(a)
}



quant5<-function(x)
  ## function gives the 25% quantile
{
  a<-quantile(x, c(0.05))
  return(a)
}

quant25<-function(x)
  ## function gives the 25% quantile
{
  a<-quantile(x, c(0.25))
  return(a)
}

quant75<-function(x)
  ## function gives the 75% quantile
{
  a<-quantile(x, c(0.75))
  return(a)
}

quant95<-function(x)
  ## function gives the 95% quantile
{
  a<-quantile(x, c(0.95))
  return(a)
}

quant97_5<-function(x)
  ## function gives the 25% quantile
{
  a<-quantile(x, c(0.975))
  return(a)
}

myFunction_exp <- function(x, A, B)
{
  exp(B * x + A)
}

myErrorFunction_exp <- function(x, A, B, RMSE)
{
  exp(B * x + A)*B*RMSE
}

myFunction_linear <- function(x, A, B)
{
  B * x + A
}

myErrorFunction_linear <- function(B, RMSE)
{
  B * RMSE
}

myFunction_pot <- function(x, A, B)
{
  A * x^B
}

myErrorFunction_pot <- function(x, A, B, RMSE)
{
  A * B * x^(B-1) * RMSE
}

myFunction_rcp <- function(x, A, B)
{
  A/x + B
}

myErrorFunction_rcp <- function(x, A, RMSE)
{
  -A/(x^2) * RMSE
}

myFunction_log <- function(x, A, B)
{
  B * ln(x) + A
}

myErrorFunction_log <- function(x, B, RMSE)
{
  B/x * RMSE
}

myFunction_polynom <- function(x, A, B)
{
  B * x^2 + A
}

myErrorFunction_polynom <- function(x, B, RMSE)
{
  2 * B * x * RMSE
}

myFunction_seasonal <- function(x, A, B)
{
  c(A,B)[x]
}

myErrorFunction_seasonal <- function(x, RMSE_A, RMSE_B)
{
  c(RMSE_A,RMSE_B)[x]
}

myFunction_quarterly <- function(x, A, B, C, D)
{
  c(A,B,C,D)[x]
}

myErrorFunction_quarterly <- function(x, RMSE_A, RMSE_B, RMSE_C, RMSE_D)
{
  c(RMSE_A,RMSE_B,RMSE_C,RMSE_D)[x]
}

ln <- function(x)
{
  log(x, base = exp(1))
}

annual_mean_conc_method1 <- function(x_in, x_out_mean, x_out_stdev, site_names)
{
  for (site_name in site_names) {

    selected <- (x_in$SiteCode == site_name)

    values <- x_in$DataValue[selected]
    BY <- list(x_in$VariableID[selected])

    x_out_mean [[site_name]] <- aggregate(values, by=BY, FUN="mean")[,2]
    x_out_stdev[[site_name]] <- aggregate(values, by=BY, FUN="sd")[,2]

  }

  list(
    mean = x_out_mean,
    stdev = x_out_stdev)

}

annual_mean_conc_method2 <- function(x_out_mean, x_out_stdev, site_names, data.dir)
{

  #read correlation info and rain series
  if (file.exists(file.path(data.dir, "correlations_substances.csv"))) {
    x_correlations <- read.table(file = file.path(data.dir, "correlations_substances.csv"), sep = ";", dec = ".", stringsAsFactors=FALSE, header = TRUE, comment.char = "")
  } else stop("File with rain runoff (correlations_substances.csv) not found in data.dir")

  if (file.exists(file.path(data.dir, "RainEvents_1961_1990_Dahlem.csv"))) {
    x_rain_events <- read.table(file = file.path(data.dir, "RainEvents_1961_1990_Dahlem.csv"), sep = ";", dec = ".", stringsAsFactors=FALSE, header = TRUE)
  } else stop("File with rain runoff (RainEvents_1961_1990_Dahlem.csv) not found in data.dir")

  #keep only lines active for model calculations
  x_correlations <- x_correlations[which(x_correlations$active == "x"),]

  #add VariableName
  variables <- OGRE_VARIABLES()
  indices <- match(x_correlations$VariableCode, variables$VariableCode)
  x_correlations$VariableName <- variables$VariableName[indices]

  #change column names in rain vents to match names of independents
  x_rain_events <- hsRenameColumns(dframe = x_rain_events, renames = list(
    "mean" = "RImean",
    "max" = "RImax",
    "sum" = "RD",
    "dur" = "DUR",
    "pBefore" = "DWP"))

  #remove rain events with value lower than threshold in mm
  indices <- which(x_rain_events$RD > 0.8)
  x_rain_events <- x_rain_events[indices,]

  #change unit from s to h
  x_rain_events$DUR <- x_rain_events$DUR/3600
  x_rain_events$DWP <- x_rain_events$DWP/3600

  #add season and quarters to rain events
  x_rain_events$tBeg <- as.POSIXct(x_rain_events$tBeg)
  x_rain_events$tEnd <- as.POSIXct(x_rain_events$tEnd)

  x_rain_events$quarter <- quarters(x_rain_events$tBeg)
  x_rain_events$quarter[which(x_rain_events$quarter == "Q1")] <- 1
  x_rain_events$quarter[which(x_rain_events$quarter == "Q2")] <- 2
  x_rain_events$quarter[which(x_rain_events$quarter == "Q3")] <- 3
  x_rain_events$quarter[which(x_rain_events$quarter == "Q4")] <- 4
  x_rain_events$quarter <- as.numeric(x_rain_events$quarter)

  indices <- which(x_rain_events$quarter == 2 | x_rain_events$quarter == 3)
  x_rain_events$season <- x_rain_events$quarter
  #summer = 1
  x_rain_events$season[indices] <- 1
  #winter = 2
  x_rain_events$season[-indices] <- 2

  #define functions
  functionName <- list(
    exp = "myFunction_exp",
    linear = "myFunction_linear",
    log = "myFunction_log",
    polynom = "myFunction_polynom",
    pot = "myFunction_pot",
    quarterly = "myFunction_quarterly",
    rcp = "myFunction_rcp",
    seasonal = "myFunction_seasonal"
  )

  #define error functions
  ErrorfunctionName <- list(
    exp = "myErrorFunction_exp",
    linear = "myErrorFunction_linear",
    log = "myErrorFunction_log",
    polynom = "myErrorFunction_polynom",
    pot = "myErrorFunction_pot",
    quarterly = "myErrorFunction_quarterly",
    rcp = "myErrorFunction_rcp",
    seasonal = "myErrorFunction_seasonal"
  )

  # calculate mean and error per site and variable

  for (site_name in site_names) {

    #select correlations for site_name
    indices <- which(x_correlations$SiteCode == site_name)
    x_by_site <- x_correlations[indices,]

    #one correlation at a time for site_name
    for (i in 1:length(x_by_site$VariableName)) {

      #parametrisation for correlation function
      functionCode <- x_by_site$FunctionCode[i]
      independent <- x_rain_events[[x_by_site$x[i]]]
      A <- x_by_site$A[i]
      B <- x_by_site$B[i]
      C <- x_by_site$C[i]
      D <- x_by_site$D[i]

      args <- list(x = independent, A = A, B = B)

      if (!is.na(C)) {
        args <- c(args, C = C, D = D)
      }

      #application correlation i
      x_rain_events$conc <- do.call(
        what = functionName[[functionCode]],
        args = args
      )

      #calculate annual mean
      x_rain_events$C_times_RD <- x_rain_events$conc*x_rain_events$RD

      #select affected VariableName in output file
      indices_out <- match(x_by_site$VariableName[i], x_out_mean$VariableName)

      x_out_mean[[site_name]][indices_out] <- sum(x_rain_events$C_times_RD, na.rm = TRUE)/sum(x_rain_events$RD, na.rm = TRUE)

      #parametrisation for error function
      RMSE <- x_by_site$RMSE[i]
      RMSE_A <- x_by_site$RMSE_A[i]
      RMSE_B <- x_by_site$RMSE_B[i]
      RMSE_C <- x_by_site$RMSE_C[i]
      RMSE_D <- x_by_site$RMSE_D[i]

      if (functionCode == "exp" | functionCode == "pot") {
        args <- list(x = independent, A = A, B = B, RMSE = RMSE)
      }
      if (functionCode == "log" | functionCode == "polynom") {
        args <- list(x = independent, B = B, RMSE = RMSE)
      }
      if (functionCode == "rcp") {
        args <- list(x = independent, A = A, RMSE = RMSE)
      }
      if (functionCode == "linear") {
        args <- list(B = B, RMSE = RMSE)
      }
      if (functionCode == "seasonal") {
        args <- list(x = independent, RMSE_A = RMSE_A, RMSE_B = RMSE_B)
      }
      if (functionCode == "quarterly") {
        args <- list(x = independent, RMSE_A = RMSE_A, RMSE_B = RMSE_B,
                     RMSE_C = RMSE_C, RMSE_D = RMSE_D)
      }

      #application error function i
      x_rain_events$deltaC <- do.call(
        what = ErrorfunctionName[[functionCode]],
        args = args
      )

      #calculate standard deviation of annual mean
      x_rain_events$dC_times_RD <- x_rain_events$deltaC*x_rain_events$RD

      x_out_stdev[[site_name]][indices_out] <- sqrt(sum(x_rain_events$dC_times_RD^2, na.rm = TRUE))/
        sum(x_rain_events$RD, na.rm = TRUE)

    }

  }

  list(
    mean = x_out_mean,
    stdev = x_out_stdev)

}

